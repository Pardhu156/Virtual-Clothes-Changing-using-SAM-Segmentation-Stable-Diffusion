# üëó Virtual Clothes Changing using SAM Segmentation & Stable Diffusion

This repository implements a **virtual clothes-changing system** using:

- **SAM (Segment Anything Model)** for high-accuracy clothing segmentation  
- **Stable Diffusion (Img2Img)** for generating new clothing styles  
- Image preprocessing & mask refinement  
- End-to-end pipeline to replace the clothing on a person while keeping facial identity intact  

---

## üìå Project Overview

The goal of this project is to:

- Detect and extract the clothing region (shirt/top/jacket) from an image  
- Generate a mask using **Meta‚Äôs SAM model**  
- Replace the segmented region using **Stable Diffusion**  
- Maintain:
  - Original background  
  - Person‚Äôs pose  
  - Face structure  
- Produce a realistic output where only the clothes are changed  

This solution can be used for:
- Virtual try-on  
- Fashion editing  
- E-commerce previews  
- Digital content creation  

---

## üß© Pipeline Summary

### **1Ô∏è‚É£ Image Input**
User uploads an image of a person.

### **2Ô∏è‚É£ Segmentation using SAM**
- SAM automatically segments the region of interest  
- IOU ~ **0.90‚Äì0.95+** for high accuracy  
- Mask is extracted and cleaned  

### **3Ô∏è‚É£ Mask Refinement**
- Convert mask to binary  
- Optionally dilate/erode to avoid leaks  
- Prepare masked area for the diffusion model  

### **4Ô∏è‚É£ Stable Diffusion Editing**
- Use Stable Diffusion Img2Img / Inpainting  
- Prompt example:
